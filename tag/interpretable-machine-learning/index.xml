<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Interpretable Machine Learning | Haiping Lu</title><link>https://haipinglu.github.io/tag/interpretable-machine-learning/</link><atom:link href="https://haipinglu.github.io/tag/interpretable-machine-learning/index.xml" rel="self" type="application/rss+xml"/><description>Interpretable Machine Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2025 by Haiping Lu</copyright><lastBuildDate>Wed, 16 Nov 2022 00:00:00 +0000</lastBuildDate><image><url>https://haipinglu.github.io/media/icon_huc5fd9f1b4f03f2d64add7afcb374c1e6_29577_512x512_fill_lanczos_center_3.png</url><title>Interpretable Machine Learning</title><link>https://haipinglu.github.io/tag/interpretable-machine-learning/</link></image><item><title>Uncertainty estimation for landmark localisation</title><link>https://haipinglu.github.io/project/landmark-uncertainty/</link><pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project/landmark-uncertainty/</guid><description>&lt;p>Automatic anatomical landmark localisation has made great strides by leveraging deep learning methods in recent years. The ability to quantify the uncertainty of these predictions is a vital ingredient needed to see these methods adopted in clinical use, where it is imperative that erroneous predictions are caught and corrected.&lt;/p>
&lt;p>We propose Quantile Binning, a data-driven method to categorise predictions by uncertainty with estimated error bounds. This framework can be applied to any continuous uncertainty measure, allowing straightforward identification of the best subset of predictions with accompanying estimated error bounds. We facilitate easy comparison between uncertainty measures by constructing two evaluation metrics derived from Quantile Binning. We demonstrate this framework by comparing and contrasting three uncertainty measures (a baseline, the current gold standard, and a proposed method combining aspects of the two), across two datasets (one easy, one hard) and two heatmap-based landmark localisation model paradigms (U-Net based and patch-based). We conclude by illustrating how filtering out gross mispredictions caught in our Quantile Bins significantly improves the proportion of predictions under an acceptable error threshold, and offer recommendations on which uncertainty measure to use and how to use it.&lt;/p></description></item><item><title>Mixed-order spectral clustering for networks</title><link>https://haipinglu.github.io/project/mixed-order-clustering/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project/mixed-order-clustering/</guid><description>&lt;p>Spectral clustering (SC) is a popular approach for gaining insights from complex networks. Conventional SC focuses on second-order structures (e.g. edges) without direct consideration of higher-order structures (e.g. triangles). This has motivated SC extensions that directly consider higher-order structures. However, both approaches are limited to considering a single order.&lt;/p>
&lt;p>To address this issue, we propose a novel Mixed-Order Spectral Clustering (MOSC) framework to model both second-order and third-order structures simultaneously. To model mixed-order structures, we propose two new methods based on Graph Laplacian (GL) and Random Walks (RW). MOSC-GL combines edge and triangle adjacency matrices, with theoretical performance guarantee. MOSC-RW combines first-order and second-order random walks for a probabilistic interpretation. Moreover, we design mixed-order cut criteria to enable existing SC methods to preserve mixed-order structures, and develop new mixed-order evaluation metrics for structure-level evaluation. Experiments on community detection and superpixel segmentation show (1) the superior performance of the MOSC methods over existing SC methods, (2) enhanced performance of conventional SC due to mixed-order cut criteria, and (3) new insights of output clusters offered by the mixed-order evaluation metrics.&lt;/p></description></item><item><title>Multisite brain fMRI classification</title><link>https://haipinglu.github.io/project/brain-fmri-multisite/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project/brain-fmri-multisite/</guid><description>&lt;p>Autism spectrum disorder (ASD) has no objective diagnosis method despite having a high prevalence. Machine learning has been widely used to develop classification models for ASD using neuroimaging data. Recently, studies have shifted towards using large multi-site neuroimaging datasets to boost the clinical applicability and statistical power of results. However, the classification performance is hindered by the heterogeneous nature of agglomerative datasets.&lt;/p>
&lt;p>In this project, we propose new methods for multi-site autism classification using the Autism Brain Imaging Data Exchange (ABIDE) dataset. We firstly propose a new second-order measure of functional connectivity (FC) named as Tangent Pearson embedding to extract better features for classification. Then we assess the statistical dependence between acquisition sites and FC features, and apply a domain adaptation approach to minimise the site dependence of FC features to improve classification. Our analysis shows that 1) statistical dependence between site and FC features is statistically significant at the 5% level, and 2) extracting second-order features from neuroimaging data and minimising their site dependence can improve over state-of-the-art classification results on the ABIDE dataset, achieving a classification accuracy of 73%.&lt;/p></description></item><item><title>Interpretable ML for Cardiac MRI</title><link>https://haipinglu.github.io/project-removed/ml-assess-cmr/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project-removed/ml-assess-cmr/</guid><description>&lt;p>This project is a &lt;a href="https://grantnav.threesixtygiving.org/grant/360G-Wellcome-215799_Z_19_Z" target="_blank" rel="noopener">Wellcome Trust Innovator Awards: Digital Technologies&lt;/a>.&lt;/p>
&lt;p>Cardiovascular diseases account for 26% of deaths in the UK. Current clinical imaging assessments rely on manual or semi-automated measurements. Emerging approaches focus on individual parts of the heart. We have developed the first tensor-based machine learning approach that holistically assesses the heart and surrounding structures on cardiovascular magnetic resonance imaging (CMRI) scans. We will develop this approach into a tool that can identify patients who respond to treatment or who will die early. Key advantages are rapid holistic assessment, minimal human error and full transparency with direct visualisation of features for the disease. We will assemble a large cohort of CMRI scans from 5, 000 patients with pulmonary hypertension, a severe condition affecting the heart, and assess the ability to predict treatment response and likelihood of early death. This tool will revolutionise disease assessment, and improve treatment delivery and patient care.&lt;/p></description></item><item><title>Learnable GCN aggregator</title><link>https://haipinglu.github.io/project/learnable-gcn-aggregator/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project/learnable-gcn-aggregator/</guid><description>&lt;p>Neighborhood aggregation is a key step in Graph Convolutional Networks (GCNs) for graph representation learning. Two commonly used aggregators, sum and mean, are designed with the homophily assumption that connected nodes are likely to share the same label. However, real-world graphs are noisy and adjacent nodes do not necessarily imply similarity.Learnable aggregators are proposed in Graph Attention Network (GAT) and Learnable Graph Convolutional Layer (LGCL). However, GAT considers node importance but not the importance of different features. The convolution aggregator in LGCL considers feature importance but it can not directly operate on graphs due to the irregular connectivity and lack of orderliness.&lt;/p>
&lt;p>In this work, we firstly unify the current learnable aggregators in a framework: Learnable Aggregator for GCN (LA-GCN) by introducing a shared auxiliary model that provides a customized schema in neighborhood aggregation. Under this framework, we propose a new model called LA-GCNMask consisting of a new aggregator function,mask aggregator. The auxiliary model learns a specific mask for each neighbor of a given node, allowing both node-level and feature-level attention. This mechanism learns to assign different importance to both nodes and features for prediction, which provides interpretable explanations for prediction and increases the model robustness. Experiments on seven graphs for node classification and graph classification tasks show that LA-GCNMask outperforms the state-of-the-art methods. Moreover, our aggregator can identify both the important nodes and node features simultaneously, which provides a quantified understanding of the relationship between input nodes and the prediction. We further conduct experiments on noisy graphs to evaluate the robustness of our model. Experiments show that LA-GCNMask consistently outperforms the state-of-the-art methods, with up to 15% improvements in terms of accuracy compared to the second best.&lt;/p></description></item><item><title>Learn via tensor modelling</title><link>https://haipinglu.github.io/project/learn-via-tensor/</link><pubDate>Mon, 16 Dec 2013 00:00:00 +0000</pubDate><guid>https://haipinglu.github.io/project/learn-via-tensor/</guid><description>&lt;p>&lt;a href="https://www.routledge.com/Multilinear-Subspace-Learning-Dimensionality-Reduction-of-Multidimensional/Lu-Plataniotis-Venetsanopoulos/p/book/9781439857243" target="_blank" rel="noopener">Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data&lt;/a> is a book published December 16, 2013 by Chapman and Hall/CRC in the &lt;a href="https://www.routledge.com/Chapman--HallCRC-Machine-Learning--Pattern-Recognition/book-series/CRCMACLEAPAT" target="_blank" rel="noopener">Chapman &amp;amp; Hall/CRC Press Machine Learning and Pattern Recognition Series&lt;/a>. This page provides an overview of resources concerned with theories and applications of multilinear subspace learning (MSL).
&lt;img style="height:260px; float: right; margin-right: 10px" src="MSLbook_CRC_Cover.png" alt="Multilinear Subspace Learning Book">&lt;/p>
&lt;p>The origin of MSL traces back to multi-way analysis in the 1960s and they have been studied extensively in face and gait recognition. With more connections revealed and analogies drawn between multilinear algorithms and their linear counterparts, MSL has become an exciting area to explore for applications involving large-scale multidimensional
(tensorial) data as well as a challenging problem for machine learning researchers to tackle.&lt;/p>
&lt;hr>
&lt;p>[ &lt;a href="#books">Book/Survey&lt;/a> | &lt;a href="#code">Software&lt;/a> | &lt;a href="#data">Data&lt;/a> | &lt;a href="#related">Related Sites&lt;/a> | &lt;a href="#references">Research Papers&lt;/a> ]&lt;/p>
&lt;p>&lt;a id='books'>&lt;/a>&lt;/p>
&lt;h2 id="a-idbooksdbook-and-surveya">&lt;a id='booksd'>Book and Survey&lt;/a>&lt;/h2>
&lt;p>&lt;a href="http://www.crcpress.com/product/isbn/9781439857243" target="_blank" rel="noopener">Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data&lt;/a>, &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/" target="_blank" rel="noopener">Haiping Lu&lt;/a>, &lt;a href="http://www.dsp.utoronto.ca/~kostas/" target="_blank" rel="noopener">K. N. Plataniotis&lt;/a>, and &lt;a href="http://www.dsp.toronto.edu/~anv/" target="_blank" rel="noopener">A. N. Venetsanopoulos&lt;/a>, Chapman &amp;amp; Hall/CRC Press Machine Learning and Pattern Recognition Series, Taylor and Francis, ISBN: 978-1-4398572-4-3, December 2013.&lt;/p>
&lt;p>Order: &lt;a href="http://www.amazon.com/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.com&lt;/a>,
&lt;a href="http://www.amazon.ca/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.ca&lt;/a>,
&lt;a href="http://www.amazon.co.uk/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.co.uk&lt;/a>,
&lt;a href="http://www.amazon.fr/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.fr&lt;/a>,
&lt;a href="http://www.amazon.de/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.de&lt;/a>,
&lt;a href="http://www.amazon.co.jp/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">Amazon.co.jp&lt;/a>,
&lt;a href="http://www.amazon.cn/Multilinear-Subspace-Learning-Dimensionality-Multidimensional/dp/1439857245" target="_blank" rel="noopener">亚马逊中国&lt;/a>,
&lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/files/MSLbook_CRC2013.pdf" target="_blank" rel="noopener">CRC Press (Save 20%)&lt;/a>&lt;br>
Contents: &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLbook-TOC.pdf" target="_blank" rel="noopener">Table of Contents&lt;/a>;
&lt;a href="http://books.google.com.hk/books?id=4F_vAgAAQBAJ&amp;amp;printsec=frontcover&amp;amp;dq=isbn:1439857245&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=1kG7U4uqD5L38QXc3oDICw&amp;amp;redir_esc=y#v=onepage&amp;amp;q&amp;amp;f=false" target="_blank" rel="noopener">Preview&lt;/a>&lt;br>
Sample Chapters: &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLbook-Chapter1.pdf" target="_blank" rel="noopener">Chapter 1&lt;/a>, &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLbook-Chapter3.pdf" target="_blank" rel="noopener">Chapter
3&lt;/a>&lt;br>
Review: Recommended at computingreviews.com (&lt;a href="http://www.computingreviews.com/review/review_review.cfm?review_id=142849" target="_blank" rel="noopener">Review Link&lt;/a>;
&lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLbook-ComputingReview.pdf" target="_blank" rel="noopener">PDF&lt;/a>)&lt;br>
Errata/Corrections: &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLBookErrata.pdf" target="_blank" rel="noopener">Errata - 19 Jan 2015&lt;/a>, &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/MSL/MSLbookCorrections20140215.pdf" target="_blank" rel="noopener">Corrections -15 Feb
2014&lt;/a>&lt;br>
Software: &lt;a href="#code">Open Source Software&lt;/a>&lt;br>
Data: &lt;a href="#data">2D face data &amp;amp; 3D gait data&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf" target="_blank" rel="noopener">A Survey of Multilinear Subspace Learning for Tensor
Data&lt;/a>, Haiping Lu, K. N. Plataniotis, and A. Venetsanopoulos, Pattern Recognition, Vol. 44, No. 7, pp. 1540-1551, Jul. 2011.&lt;/p>
&lt;h2 id="a-idcodesoftwarea">&lt;a id='code'>Software&lt;/a>&lt;/h2>
&lt;p>Open source software on multilinear subspace learning algorithms:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="http://csmr.ca.sandia.gov/%7Etgkolda/TensorToolbox/" target="_blank" rel="noopener">Matlab Tensor Toolbox&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.mathworks.com/matlabcentral/fileexchange/26168" target="_blank" rel="noopener">MPCA&lt;/a>: the multilinear principal component analysis algorithm, a multilinear extension of PCA, including code, data and paper.&lt;/li>
&lt;li>&lt;a href="http://www.mathworks.com/matlabcentral/fileexchange/35432" target="_blank" rel="noopener">UMPCA&lt;/a>: the uncorrelated multilinear principal component analysis algorithm, including code, data and paper.&lt;/li>
&lt;li>&lt;a href="http://www.mathworks.com/matlabcentral/fileexchange/35782" target="_blank" rel="noopener">UMLDA&lt;/a> : the uncorrelated multilinear discriminant analysis algorithm, including code, data and paper.&lt;/li>
&lt;/ul>
&lt;h2 id="a-iddatadataa">&lt;a id='data'>Data&lt;/a>&lt;/h2>
&lt;p>The FERET face data (2-D tensor, i.e, matrix) and training/test
partitions:&lt;br>
C=number of subjects; A=max angle;S: number of samples/subject&lt;br>
&lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/feretc70a15s8.zip" target="_blank" rel="noopener">C70A15S8 (3.09M)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/feretc80a45s6.zip" target="_blank" rel="noopener">C80A45S6
(893K)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/feretc160a45s6.zip" target="_blank" rel="noopener">C160A45S6
(1.32M)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/feretc240a45s6.zip" target="_blank" rel="noopener">C240A45S6
(1.68M)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/feretc320a45s6.zip" target="_blank" rel="noopener">C320A45S6
(2.04M)&lt;/a>.&lt;/p>
&lt;p>The CMU PIE face data (2-D tensor, i.e, matrix) and training/test
partitions: &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/piep3i3.zip" target="_blank" rel="noopener">PIEP3I3 (10.2M)&lt;/a>.&lt;/p>
&lt;p>The USF gait data version 1.7 (3-D tensor): &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/USFGait17_128x88x20.zip" target="_blank" rel="noopener">128x88x20
(21.2M)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/USFGait17_64x44x20.zip" target="_blank" rel="noopener">64x44x20
(9.9M)&lt;/a>; &lt;a href="http://staffwww.dcs.shef.ac.uk/people/H.Lu/CodeData/USFGait17_32x22x10.zip" target="_blank" rel="noopener">32x22x10
(3.2M)&lt;/a>.&lt;/p>
&lt;h2 id="a-idrelatedrelated-web-sitesa">&lt;a id='related'>Related Web Sites&lt;/a>&lt;/h2>
&lt;p>&lt;a href="http://en.wikipedia.org/wiki/Multilinear_subspace_learning" target="_blank" rel="noopener">Wikipedia entry&lt;/a> on Multilinear Subspace Learning. &lt;/p>
&lt;h2 id="a-idreferencesbibliographya">&lt;a id='references'>Bibliography&lt;/a>&lt;/h2>
&lt;p>Papers relevant to MSL are ordered below according to topic, with
occasional papers occurring under multiple headings.&lt;/p>
&lt;hr>
&lt;p>[ &lt;a href="#tut">Tutorials&lt;/a> |  &lt;a href="#ttp">Tensor2Tensor&lt;/a> | &lt;a href="#tvp">Tensor2Vector&lt;/a> ]&lt;/p>
&lt;h3 id="a-idtuttutorialsa">&lt;a id='tut'>Tutorials&lt;/a>&lt;/h3>
&lt;p>Tutorial materials suitable for a first introduction to MSL.
Prerequisites: elementary probability theory, statistics and linear
algebra.&lt;/p>
&lt;p>H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &lt;a href="http://www.crcpress.com/product/isbn/9781439857243" target="_blank" rel="noopener">Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional
Data&lt;/a>, Chapman &amp;amp; Hall/CRC Press Machine Learning and Pattern Recognition Series, Taylor and Francis, ISBN: 978-1-4398572-4-3, 2013.&lt;/p>
&lt;p>H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &lt;a href="http://www.dsp.utoronto.ca/%7Ehaiping/Publication/SurveyMSL_PR2011.pdf" target="_blank" rel="noopener">A Survey of Multilinear Subspace Learning for Tensor Data&lt;/a>,
Pattern Recognition, Vol. 44, No. 7, pp. 1540-1551, Jul.
2011.&lt;/p>
&lt;p>T. G. Kolda, B. W. Bader, &lt;a href="http://portal.acm.org/citation.cfm?id=1655230" target="_blank" rel="noopener">Tensor decompositions and applications&lt;/a>, SIAM Review, Vol. 51, No. 3, pp. 455-500, 2009.&lt;/p>
&lt;p>L. D. Lathauwer, B. D. Moor, J. Vandewalle, &lt;a href="http://portal.acm.org/citation.cfm?id=354405" target="_blank" rel="noopener">On the best rank-1 and rank-(R1, R2, ..., RN ) approximation of higher-order tensors&lt;/a>,
SIAM Journal of Matrix Analysis and Applications 21 (4) (2000)
1324-1342.&lt;/p>
&lt;p>L.D. Lathauwer, B.D. Moor, J. Vandewalle, &lt;a href="http://portal.acm.org/citation.cfm?id=354398" target="_blank" rel="noopener">A multilinear singular value decomposition&lt;/a>,
SIAM Journal of Matrix Analysis and Applications vol. 21, no. 4, pp.
1253-1278, 2000.&lt;/p>
&lt;h3 id="a-idttpmsl-through-tensor-to-tensor-projectiona">&lt;a id='ttp'>MSL through Tensor-to-Tensor Projection&lt;/a>&lt;/h3>
&lt;p>MSL algorithms that project a tensor directly to another tensor of
lower dimension.&lt;/p>
&lt;p>H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &lt;a href="http://www.dsp.utoronto.ca/%7Ehaiping/Publication/MPCA_TNN08_rev2010.pdf" target="_blank" rel="noopener">MPCA: Multilinear Principal Component Analysis of Tensor
Objects&lt;/a>, IEEE Trans. on Neural Networks, Vol. 19, No. 1, Page: 18-39, Jan. 2008.&lt;/p>
&lt;p>D. Tao, X. Li, X. Wu, and S. J. Maybank, &lt;a href="http://dx.doi.org/10.1109/TPAMI.2007.1096" target="_blank" rel="noopener">General tensor discriminant analysis and gabor features for gait recognition&lt;/a>, IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 10, pp. 1700-1715, Oct. 2007.&lt;/p>
&lt;p>S. Yan, D. Xu, Q. Yang, L. Zhang, X. Tang, and H.-J. Zhang, &lt;a href="http://portal.acm.org/citation.cfm?id=1068959" target="_blank" rel="noopener">Discriminant analysis with tensor representation&lt;/a>, in Proc. IEEE Conference on Computer Vision and Pattern Recognition, vol. I, June 2005, pp. 526-532.&lt;/p>
&lt;p>X. He, D. Cai, P. Niyogi, &lt;a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0249.pdf" target="_blank" rel="noopener">Tensor subspace analysis&lt;/a>], in: Advances in Neural Information Processing Systemsc 18 (NeurIPS), 2005.&lt;/p>
&lt;h3 id="a-idtvpmsl-through-tensor-to-vector-projectiona">&lt;a id='tvp'>MSL through Tensor-to-Vector Projection&lt;/a>&lt;/h3>
&lt;p>MSL algorithms that project a tensor directly to a vector of lower
dimension.&lt;/p>
&lt;p>H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &lt;a href="http://www.dsp.utoronto.ca/%7Ehaiping/Publication/UMPCA_TNN09.pdf" target="_blank" rel="noopener">Uncorrelated Multilinear Principal Component Analysis for Unsupervised Multilinear Subspace Learning&lt;/a>, IEEE Trans. on Neural Networks, Vol. 20, No. 11, Page: 1820-1836, Nov. 2009.&lt;/p>
&lt;p>H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, &lt;a href="http://www.dsp.utoronto.ca/%7Ehaiping/Publication/UMLDA_TNN08.pdf" target="_blank" rel="noopener">Uncorrelated Multilinear Discriminant Analysis with Regularization and
Aggregation for Tensor Object Recognition&lt;/a>, IEEE Trans. on Neural Networks, Vol. 20, No. 1, Page: 103-123, Jan. 2009.&lt;/p>
&lt;hr>
&lt;hr>
&lt;p>This page is maintained by &lt;a href="https://haipinglu.github.io/" target="_blank" rel="noopener">Haiping Lu&lt;/a> and its layout design follows &lt;a href="http://www.gaussianprocess.org" target="_blank" rel="noopener">www.gaussianprocess.org&lt;/a>. Please send suggestions for corrections or additions via email to: hplu [at] ieee [dot] org.&lt;br>
Latest update: December 30, 2021.&lt;/p></description></item></channel></rss>